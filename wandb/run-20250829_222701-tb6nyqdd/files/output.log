|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 281190 KiB | 281190 KiB | 560310 KiB | 279119 KiB |
|       from large pool | 229258 KiB | 279119 KiB | 508377 KiB | 279119 KiB |
|       from small pool |  51932 KiB |  51932 KiB |  51932 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 281190 KiB | 281190 KiB | 560310 KiB | 279119 KiB |
|       from large pool | 229258 KiB | 279119 KiB | 508377 KiB | 279119 KiB |
|       from small pool |  51932 KiB |  51932 KiB |  51932 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 281166 KiB | 281166 KiB | 560285 KiB | 279118 KiB |
|       from large pool | 229258 KiB | 279118 KiB | 508376 KiB | 279118 KiB |
|       from small pool |  51908 KiB |  51908 KiB |  51908 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 335872 KiB | 335872 KiB | 335872 KiB |      0 B   |
|       from large pool | 280576 KiB | 280576 KiB | 280576 KiB |      0 B   |
|       from small pool |  55296 KiB |  55296 KiB |  55296 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  54681 KiB | 208884 KiB | 251380 KiB | 196699 KiB |
|       from large pool |  51317 KiB | 206837 KiB | 208294 KiB | 156977 KiB |
|       from small pool |   3364 KiB |   4244 KiB |  43086 KiB |  39722 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     273    |     273    |     275    |       2    |
|       from large pool |      91    |      91    |      92    |       1    |
|       from small pool |     182    |     182    |     183    |       1    |
|---------------------------------------------------------------------------|
| Active allocs         |     273    |     273    |     275    |       2    |
|       from large pool |      91    |      91    |      92    |       1    |
|       from small pool |     182    |     182    |     183    |       1    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      28    |      28    |      28    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      27    |      27    |      27    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      28    |      30    |       2    |
|       from large pool |       1    |       1    |       2    |       1    |
|       from small pool |      27    |      27    |      28    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

bf16: True
  0%|                                                                                                                                | 4/61073 [00:11<47:05:03,  2.78s/it]Traceback (most recent call last):
{'loss': 18.2187, 'grad_norm': 171.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 18.2183, 'grad_norm': 173.0, 'learning_rate': 9.823182711198428e-08, 'epoch': 0.0}
{'loss': 18.1575, 'grad_norm': 171.0, 'learning_rate': 1.9646365422396855e-07, 'epoch': 0.0}
{'loss': 18.153, 'grad_norm': 165.0, 'learning_rate': 2.946954813359528e-07, 'epoch': 0.0}
  File "/home/jpaulsen/repos/mimi-smollm/train_emilia.py", line 206, in <module>
    main()
  File "/home/jpaulsen/repos/mimi-smollm/train_emilia.py", line 201, in main
    trainer.train()
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2734, in backward
    loss.backward(**kwargs)
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x77fafceb8ae0>
Traceback (most recent call last):
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/jpaulsen/repos/mimi-smollm/.venv/lib/python3.12/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
